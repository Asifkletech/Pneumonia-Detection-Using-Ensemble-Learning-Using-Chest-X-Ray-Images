# -*- coding: utf-8 -*-
"""sk1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12kgp3qmkttBXI8_jdXikjFkxvu9ZpfHS
"""

# Import necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import cv2
import os
from sklearn.metrics import classification_report, confusion_matrix

from sklearn.model_selection import train_test_split
from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential, Model
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization, Input, Concatenate
from keras.optimizers import Adam
from keras.callbacks import EarlyStopping, ModelCheckpoint
from keras.utils import plot_model
from keras.applications import Mobilenet, ResNet50, Densenet
from keras import backend as K

# Set random seed
np.random.seed(42)

# Load the dataset
dataset_path = "/content/drive/MyDrive/chest_xray"
train_dir = os.path.join(dataset_path, "train")
test_dir = os.path.join(dataset_path, "test")

# Define image size
img_width = 224
img_height = 224

# Define batch size
batch_size = 70

# Define train/validation split ratio
split_ratio = 0.2

# Define data augmentation for training data
train_datagen = ImageDataGenerator(rescale=1./255,
                                   rotation_range=10,
                                   zoom_range=0.1,
                                   horizontal_flip=True,
                                   validation_split=split_ratio)

# Define data augmentation for test data
test_datagen = ImageDataGenerator(rescale=1./255)

print('Examples - Normal')

plt.figure(figsize=(12,12))

for i in range(0, 4):
    plt.subplot(3,4,i + 1)
    img = cv2.imread(train_normal[i])
    img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))
    plt.imshow(img)
    plt.axis("off")

plt.tight_layout()

plt.show()

print('Exemples - Pneumonia')

plt.figure(figsize=(12,12))

for i in range(0, 4):
    plt.subplot(3,4,i + 1)
    img = cv2.imread(train_pneumonia[i])
    img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))
    plt.imshow(img)
    plt.axis("off")

plt.tight_layout()

plt.show()

# Generate training data
train_generator = train_datagen.flow_from_directory(train_dir,
                                                    target_size=(img_width, img_height),
                                                    batch_size=batch_size,
                                                    class_mode="binary",
                                                    subset="training")

# Generate test data
test_generator = test_datagen.flow_from_directory(test_dir,
                                                   target_size=(img_width, img_height),
                                                   batch_size=batch_size,
                                                   class_mode="binary",
                                                   shuffle=False)

# Build the model
input_shape = (img_width, img_height, 3)

# Define the input tensor
input_tensor = Input(shape=input_shape)

# Build Mobilenet model
mobilenet_model = Mobilenet(weights="imagenet", include_top=False, input_tensor=input_tensor)

# Build ResNet50 model
resnet_model = ResNet50(weights="imagenet", include_top=False, input_tensor=input_tensor)

# Build Densenet model
densenet_model = Densenet121(weights="imagenet", include_top=False, input_tensor=input_tensor)

# Combine models
x1 = Mobilenet_model.output
x1 = Flatten()(x1)

x2 = resnet_model.output
x2 = Flatten()(x2)

x3 = Densenet_model.output
x3 = Flatten()(x3)

x = Concatenate()([x1, x2, x3])
x = Dense(256, activation="relu")(x)
x = Dropout(0.5)(x)
x = Dense(1, activation="sigmoid")(x)

model = Model(inputs=input_tensor, outputs=x)

# Generate validation data
validation_generator = train_datagen.flow_from_directory(train_dir,
                                                         target_size=(img_width, img_height),
                                                         batch_size=batch_size,
                                                         class_mode="binary",
                                                         subset="validation")

# Generate test data
test_generator = test_datagen.flow_from_directory(test_dir,
                                                   target_size=(img_width, img_height),
                                                   batch_size=batch_size,
                                                   class_mode="binary",
                                                   shuffle=False)

# Compile the model
model.compile(loss="binary_crossentropy", optimizer=Adam(lr=1e-5), metrics=["accuracy"])

# Define callbacks
early_stopping = EarlyStopping(monitor="val_loss", patience=5, verbose=1)
model_checkpoint = ModelCheckpoint("model.h5", monitor="val_loss", save_best_only=True, verbose=1)

from keras.preprocessing.image import ImageDataGenerator

# Train the model
history = model.fit(train_generator,
steps_per_epoch=train_generator.samples // batch_size,
epochs=70,
validation_data=validation_generator,
validation_steps=validation_generator.samples // batch_size,
callbacks=[early_stopping, model_checkpoint])

model.load_weights("model.h5")
y_pred = model.predict(test_generator, steps=test_generator.samples // batch_size+1, verbose=1)
y_pred = np.where(y_pred > 0.5, 1, 0)
y_true = test_generator.classes

fig, ax = plt.subplots(figsize=(20,8))
sns.lineplot(x = history.epoch, y = history.history['loss'])
sns.lineplot(x = history.epoch, y = history.history['val_loss'])
ax.set_title('Learning Curve (Loss)')
ax.set_ylabel('Loss')
ax.set_xlabel('Epoch')
ax.set_ylim(0, 0.5)
ax.legend(['train', 'val'], loc='best')
plt.show()

fig, ax = plt.subplots(figsize=(20,8))
sns.lineplot(x = history.epoch, y = history.history['binary_accuracy'])
sns.lineplot(x = history.epoch, y = history.history['val_binary_accuracy'])
ax.set_title('Learning Curve (Accuracy)')
ax.set_ylabel('Accuracy')
ax.set_xlabel('Epoch')
ax.set_ylim(0.80, 1.0)
ax.legend(['train', 'val'], loc='best')
plt.show()

#Calculate evaluation metrics
report = classification_report(y_true, y_pred)
print(report)

confusion_mtx = confusion_matrix(y_true, y_pred)
sns.heatmap(confusion_mtx, annot=True, fmt="d")

tp = confusion_mtx[1,1]
tn = confusion_mtx[0,0]
fp = confusion_mtx[0,1]
fn = confusion_mtx[1,0]
recall = tp / (tp + fn)
precision = tp / (tp + fp)
f1score = 2 * precision * recall / (precision + recall)
accuracy = (tp + tn) / (tp + tn + fp + fn)

print("Recall: {:.2f}".format(recall))
print("Precision: {:.2f}".format(precision))
print("F1 Score: {:.2f}".format(f1score))
print("Accuracy: {:.2f}".format(accuracy))
plt.plot(history.history["accuracy"])
plt.plot(history.history["val_accuracy"])
plt.title("Model Accuracy")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend(["train", "val"], loc="upper left")
plt.show()

plt.plot(history.history["loss"])
plt.plot(history.history["val_loss"])
plt.title("Model Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend(["train", "val"], loc="upper left")
plt.show()